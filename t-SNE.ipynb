{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE on RAPIDS\n",
    "\n",
    "This notebook is geared towards a general understanding of t-SNE as well as the estimated benefits of using RAPIDS over traditional libraries.\n",
    "\n",
    "## Understading t-SNE\n",
    "\n",
    "***General Idea***: Takes a high dimensional data-set and creates a low-dimensional representation of the original data, so that the original clustering in the high-dimensional space is preserved.\n",
    "\n",
    "*Awesome [video](https://youtu.be/NEaUSP4YerM) from StatQuest.*\n",
    "\n",
    "### A little more in-depth\n",
    "\n",
    "Given a set of N high-dimensional data points ($x_1,x_2,....x_N$), t-SNE (T-distributed Stochastic Neighbor Embedding) can be conceptualized as a two-stage algorithm as follows:\n",
    "\n",
    "--- \n",
    "1. Construct a probability distribution over pairs of the original high-dimensional data points that rewards high probability of a point being selected due to similarlity.\n",
    "\n",
    "For each pair of points $x_i, x_j$, a symmetrized version, $p_{ji}$ of the conditional similarity is computed. This symmetrized probability is dependent on the conditional similarity,$p_{j|i}$, between the pair of points. The conditional similarity measures how \"close\" $x_j$ if from $x_i$, using a Normal distribution around $x_i$, with a variance, $\\sigma^2$ that is defined for each point. \n",
    "\\begin{equation*} p_{ij} = \\frac{p_{j|i}+p_{i|j}}{2N} \\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*} p_{j|i} = \\frac{e^{\\frac{-||x_i - x_j||^2}{2\\sigma_i^2}}}{\\sum_{k\\neq i}e^{\\frac{-||x_i - x_k||^2}{2\\sigma_i^2}}} \\end{equation*}\n",
    "\n",
    "---\n",
    "2. Construct a probability distribution over points in the low-dimensional representation, and then minimize a divergence metric (Kullback-Leibler divergence) between the two distributions with respect to the locations of the points on the low-dimensional mapping.\n",
    "\n",
    "The data points are individually mapped onto a number line and a heavy-tailed T-distribution is used to measure similarity between the low-dimensional points, so that dissimilar objects are modelled far apart in the map. This is done by mapping the original data to the lower dimensional points $y_{1},y_{2},....y_{N}$ with a similarity measure $q_{ij}$ which reflects the similarities $p_{ij}$.\n",
    "\n",
    "\\begin{equation*} q_{ij} = \\frac{(1+||y_i - y_j||^2)^{-1}}{\\sum_{k\\neq i}(1+||y_i - y_k||^2)^{-1}} \\end{equation*}\n",
    "\n",
    "The locations of the points $y_i$ are found by minimizing the [Kullback-Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) from the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
